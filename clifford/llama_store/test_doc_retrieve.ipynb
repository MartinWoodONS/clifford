{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate nodes and push to vector store\n",
    "**Note** If a VectorStore already exists on disk go straight to load indexes step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingest_emails import emails_to_documents\n",
    "\n",
    "docs = emails_to_documents(row_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import MetadataMode\n",
    "\n",
    "# See what will be fed into a LLM\n",
    "print(\"The LLM sees this: \\n\", docs[1].get_content(metadata_mode=MetadataMode.LLM))\n",
    "print(\"The Embedding model sees this: \\n\", docs[1].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_vector_store import parse_nodes_from_docs\n",
    "\n",
    "nodes = parse_nodes_from_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_vector_store import create_index, save_index\n",
    "\n",
    "# Index nodes and presist index to disk\n",
    "index = create_index(nodes)\n",
    "save_index(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load VectorStore indexes from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_vector_store import load_index\n",
    "\n",
    "index = load_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve similar docs from store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.vector_store.retrievers.retriever import VectorIndexRetriever\n",
    "\n",
    "def search_docstore(index, term):\n",
    "    vi_retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n",
    "    return vi_retriever.retrieve(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_docstore(index=index, term=\"Which docs mention meetings?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ingest_emails import emails_to_documents\n",
    "\n",
    "docs = emails_to_documents(row_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_vector_store import parse_nodes_from_docs\n",
    "\n",
    "nodes = parse_nodes_from_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import KnowledgeGraphIndex, ServiceContext\n",
    "# from llama_index.llms import OpenAI, Anthropic\n",
    "# from llama_index.storage.storage_context import StorageContext\n",
    "# from llama_index.graph_stores import SimpleGraphStore\n",
    "\n",
    "# from IPython.display import Markdown, display\n",
    "\n",
    "# # llm = OpenAI(temperature=0, model=\"text-davinci-002\")\n",
    "# llm = Anthropic(model=\"claude-2\", temperature=0, max_tokens=512)\n",
    "# service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
    "\n",
    "# graph_store = SimpleGraphStore()\n",
    "# storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# # NOTE: can take a while!\n",
    "# index = KnowledgeGraphIndex(\n",
    "#     nodes,\n",
    "#     max_triplets_per_chunk=2,\n",
    "#     storage_context=storage_context,\n",
    "#     service_context=service_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.storage_context.persist(persist_dir=\"../storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = index.as_query_engine(include_text=False, response_mode=\"tree_summarize\")\n",
    "# response = query_engine.query(\n",
    "#     \"Tell me more about Phillip\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## create graph\n",
    "# from pyvis.network import Network\n",
    "\n",
    "# g = index.get_networkx_graph()\n",
    "# net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "# net.from_nx(g)\n",
    "# net.show(\"example.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import KnowledgeGraphIndex, ServiceContext\n",
    "from llama_index.llms import OpenAI, Anthropic\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# llm = OpenAI(temperature=0, model=\"text-davinci-002\")\n",
    "llm = Anthropic(model=\"claude-2\", temperature=0, max_tokens=512)\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512)\n",
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# NOTE: can take a while!\n",
    "index = KnowledgeGraphIndex(\n",
    "    nodes,\n",
    "    max_triplets_per_chunk=2,\n",
    "    service_context=service_context,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"../storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_vector_store import create_index, save_index\n",
    "\n",
    "# Index nodes and presist index to disk\n",
    "vec_index = create_index(nodes)\n",
    "save_index(vec_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query using top 3 triplets plus keywords (duplicate triplets are removed)\n",
    "query_engine = index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What are the latest tasks that Phillip Allen has requested and who was responsible for fulfulling those tasks?\"\n",
    "query = \"What actions have been assigned by Phillip Allen, which individuals were these actions assigned to and if a date or timeframe was specified, what was it?\"\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are Phillip Allen's roles and responsibilities?\"\n",
    "response = query_engine.query(query)\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "g = index.get_networkx_graph()\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show(\"example.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clifford-py",
   "language": "python",
   "name": "clifford-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
